
# Sentiment analysis on amazon reviews with RoBERTa 
# TUTORIAL ON YOUTUBE:  https://www.youtube.com/watch?v=QpzMWQvxXWk&t=34s
# TODO DOWNLOAD THE SAME DATASET HE IS USING IN THE TUTORIAL

# STEP 0
import pandas as pd
import numpy as np
import matplotlib.style 
# instead of using: import matplotlib as plt , use the following
import matplotlib.pyplot as plt
# pip install scipy
import transformers 


plt.style.use("ggplot")
import nltk

# ------------Read in data
# terminal, cd: C:/Users/User/OneDrive/Desktop/APRENDER/Python/data_amazon_TVreviews/
df = pd.read_csv("ratings_Movies_and_TV.csv")
print("large corpora", df.shape) 
# to make our data smaller
df = df.head(500)
print("shape of smaller corpora", df.shape)


# add row to an specific index # df.loc[1.0] is useful to select the row, column etc...
df.columns= ["id","SOMETHING","Score", "somethingid"]
df= df.sort_index().reset_index(drop=True)
print(df.head())


#-------------- quick EDA
#if I want the numbers to be int insteaad of float: df= df.astype({"Score":int})
ax = df['Score'].value_counts().sort_index().plot(kind ="bar", title= "Count of Reviews by Stars", figsize=(10,5))
ax.set_xlabel("Review Stars")
print(plt.show())


# ------------ basic NLTK 
# download reviews  + column 'Text' 
example = df["Text"][50]
print(example)

tokens= nltk.word_tokenize(example)
print(tokens[:10]) # print first 10 tokens

tagged= nltk.pos_tag(tokens)
tagged[:10]


entities = nltk.chunk.ne_chunk(tagged)
entities.pprint() 


# ---------- STEP 1: VADER Sentiment scoring
# NLTK Sentiment Intensity analizer to get the ngeg, neutre and positive scores of the text
# It uses a BOW approach, stop words are removed, each word is scored and combined to a total score
from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm
sia= SentimentIntensityAnalyzer()

# some examples: 
sia.polarity_scores("I am so happy!")
sia.polarity_scores("This is the wordst thing ever")
sia.polarity_scores(example)


# run the polarity score on the entire dataset
res= {}
for i, row in tqdm(df.iterrows(), total= len(df)):
    text= row["Text"]
    myid = row["id"]
    res[myid] = sia.polarity_scores(text)


#
vaders= pd.DataFrame(res).T
# en caso de necesitarlo: vaders = vaders.reset_index().rename(columns= {"index":"id"})
vaders = vaders.merge(df, how="left")
# now wehave sentimetn score and metada
print(vaders.head())



#terminal: pip install seaborn
import seaborn as sns
# plot vaders results
ax = sns.barplot(data=vaders, x="Score", y="compound")
ax.set_title("Compound Score by Amazon Star Review")
plt.show()


##
fig, axs= plt.subplots(1,2, figsize=(12,3))
sns.barplot(data=vaders, x="Score", y="pos", ax=axs[0])
sns.barplot(data=vaders, x="Score", y="pos", ax=axs[1])
sns.barplot(data=vaders, x="Score", y="pos", ax=axs[2])
axs[0].set_title("Positive")
axs[1].set_title("Neutral")
axs[2].set_title("Negative")
plt.tight_layout()
plt.show()




# Step 3 ROBERTA

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax

MODEL= f"cadiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model= AutoModelForSequenceClassification.from_pretrained(MODEL)

# VADER results on example
print(example)
sia.polarity_scores(example)

#Run for roberta model
encoded_text= tokenizer(example,return_tensors="pt")
output= model(**encoded_text)
scores = output[0][0].detach().numpy()
scores= softmax(scores)
scores_dict= { "roberta_neg": scores[0], "roberta_neu": scores[1], "roberta_pos":scores[2]}
print(scores_dict)


# 
def polarity_scores_roberta(example):
    encoded_text = tokenizer(example, return_tensors="pt")
    output= model(**encoded_text)
    scores= output[0][0].detach().numpy()
    scores = softmax(scores)
    scores_dict= { "roberta_neg": scores[0], "roberta_neu": scores[1], "roberta_pos":scores[2]}
    return scores_dict


# 
res= {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        text= row["Text"]
        myid = row["id"]
        vader_result = sia.polarity_scores(text)
        vader_result_rename = {}
        for key, value in vader_result.items():
            vader_result_rename[f"vader_{key}"] = value
            roberta_result= polarity_scores_roberta(text)
            both= {**vader_result_rename, **roberta_result}
            res[myid] = both
        except RuntimeError:
            print(f"Broke for id {myid}")


##
results_df = pd.DataFrame(res).T
results_df = results_df.reset_index().rename(columns= {"index":"id"})
results_df = results_df.merge(df,how="left")

##COMPARE SCORES BETWEEN MODELS
results_df.columns


# STEP 3 COMBINE AND COMPARE

sns.pairplot(data= results_df, vars= ["vader_neg", "vader_neu", "vader_pos", "roberta_neg", "roberta_neu", "roberta_pos"], hue="Score", palette= "tab10")
plt.show()


# step 4 review examples
#let us look at some examples where the model scoring and review score differ the most, ex. predicting 1 star as positive, and 5 star as negative
# for positive reviews
results_df.query("Score ==1").sort_values("roberta_pos", ascending=False["Text"].values[0])
results_df.query("Score ==1").sort_values("vader_pos", ascending=False["Text"].values[0])





# extra: Transformers pipeline

from transformers import pipeline
sent_pipeline = pipeline("sentiment-analysis")
sent_pipeline("I love sentiment analysis!")

