{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9iMEoBg003y",
        "outputId": "9b889756-8dd4-471e-f2cd-f430cec48457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "2023-10-08 15:09:10.975137: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-10-08 15:09:12.257846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install requests\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EJjsGw655rF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Webscraping the Wikipedia"
      ],
      "metadata": {
        "id": "wZMbDh1--SW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK basic version (general passwords)"
      ],
      "metadata": {
        "id": "hOQVRNnL364Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Initialize spaCy and other tools\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ps = PorterStemmer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtl_PwuE2yGe",
        "outputId": "12dae393-2b60-43f9-8904-ce7ba50c45cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize spaCy and other tools\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def get_profiles_from_urls(url_list):\n",
        "    profiles = {}\n",
        "    for url in url_list:\n",
        "        r = requests.get(url)\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        name = soup.select_one('.firstHeading').text\n",
        "        paragraphs = soup.select('.mw-parser-output p')\n",
        "        profile_text = ' '.join([p.text for p in paragraphs[:]])  # take all paragraphs\n",
        "        profiles[name] = profile_text\n",
        "    return profiles\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token for token in doc if not token.is_punct and not token.is_space]\n",
        "    tokens = [token.text.lower() for token in tokens if token.text.lower() not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    named_entities = {}\n",
        "    for ent in doc.ents:\n",
        "        named_entities[ent.label_] = named_entities.get(ent.label_, [])\n",
        "        if ent.text.lower().strip():\n",
        "            named_entities[ent.label_].append(ent.text.lower())\n",
        "    return named_entities\n",
        "\n",
        "def generate_passwords(named_entities, num_passwords=10):\n",
        "    possible_elements = []\n",
        "    for category, entities in named_entities.items():\n",
        "        possible_elements.extend([e for e in entities if e.strip()])\n",
        "\n",
        "    passwords = []\n",
        "    for _ in range(num_passwords):\n",
        "        if len(possible_elements) >= 3:\n",
        "            elements_to_use = random.sample(possible_elements, 3)\n",
        "            random.shuffle(elements_to_use)\n",
        "            password = \"\".join(elements_to_use)\n",
        "            if random.choice([True, False]):\n",
        "                password += str(random.randint(0, 99))\n",
        "            passwords.append(password)\n",
        "        else:\n",
        "            passwords.append(\"Insufficient data\")\n",
        "\n",
        "    return passwords\n",
        "\n"
      ],
      "metadata": {
        "id": "jiC-ivw92ddN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List of Wikipedia URLs for notable people who died before 1980\n",
        "urls = [\n",
        "    'https://en.wikipedia.org/wiki/Albert_Einstein',\n",
        "    'https://en.wikipedia.org/wiki/Isaac_Newton',\n",
        "    'https://en.wikipedia.org/wiki/Galileo_Galilei',\n",
        "    # ... add more URLs\n",
        "]\n",
        "\n",
        "# Step 1: Scrape Wikipedia\n",
        "profiles = get_profiles_from_urls(urls)\n",
        "\n",
        "# Step 2: Preprocess and Generate Passwords\n",
        "for name, profile_text in profiles.items():\n",
        "    print(f\"Generating possible passwords for {name}\")\n",
        "    preprocessed_text = preprocess_text(profile_text)\n",
        "    named_entities = extract_named_entities(preprocessed_text)\n",
        "    passwords = generate_passwords(named_entities)\n",
        "    print(\"Generated Passwords:\")\n",
        "    for i, password in enumerate(passwords, 1):\n",
        "        print(f\"{i}. {password}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgyfRqOP2dau",
        "outputId": "13124f62-6bf2-4ab0-d32a-ed9ee4dd892c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating possible passwords for Albert Einstein\n",
            "Generated Passwords:\n",
            "1. 5][229united statesmach\n",
            "2. 27institute agriculture chemical institute institute microbiologybrazil66\n",
            "3. oskar halecki giuseppegerman1935\n",
            "4. seventhroosevelt recommendingfive\n",
            "5. 1908one191674\n",
            "6. brownian1905claim.[255\n",
            "7. 1922 1932hans albertfirst91\n",
            "8. quantum194one\n",
            "9. ethel michanowski berlinmax planck nielsone38\n",
            "10. albert hallnew york 's1937 edition\n",
            "\n",
            "\n",
            "Generating possible passwords for Isaac Newton\n",
            "Generated Passwords:\n",
            "1. 1999theologiancharles ii\n",
            "2. first1988isaac barrow49\n",
            "3. thirty yearsma.[22 time169920\n",
            "4. 10,000 years agoorator university cambridgeknight strength\n",
            "5. newtongodoxford university museum natural\n",
            "6. londonsixteen years144 june 2020\n",
            "7. 2001.[156john locke1665–166688\n",
            "8. 149newtonorator university cambridge36\n",
            "9. august 1665newton170143\n",
            "10. mathematician nicolas fatio de duillier 16911701woolsthorpe colsterworth\n",
            "\n",
            "\n",
            "Generating possible passwords for Galileo Galilei\n",
            "Generated Passwords:\n",
            "1. 1737twoscripture.[250\n",
            "2. galileoitalianthree84\n",
            "3. newtonlatinpope\n",
            "4. galileogalileothree\n",
            "5. galileofirstsixth\n",
            "6. netherlands1617.[7740 years27\n",
            "7. galilei jr.1616huygens 1650s.[citation\n",
            "8. galileogalileogalileo74\n",
            "9. galileoaristoteliangalileo\n",
            "10. galileoaristotelianwritings[144 september 163244\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFur71Kq1Y3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qf6xf7me1Y0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhxgR8xz1YyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK - easy & medium & difficult password generation"
      ],
      "metadata": {
        "id": "pDSn17MX399H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import random\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "\n",
        "# Initialize spaCy and other tools\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def get_profiles_from_urls(url_list):\n",
        "    profiles = {}\n",
        "    for url in url_list:\n",
        "        r = requests.get(url)\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        name = soup.select_one('.firstHeading').text\n",
        "        paragraphs = soup.select('.mw-parser-output p')\n",
        "        profile_text = ' '.join([p.text for p in paragraphs[:5]])  # Taking the first 5 paragraphs\n",
        "        profiles[name] = profile_text\n",
        "    return profiles\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token for token in doc if not token.is_punct and not token.is_space]\n",
        "    tokens = [token.text.lower() for token in tokens if token.text.lower() not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    named_entities = {}\n",
        "    for ent in doc.ents:\n",
        "        named_entities[ent.label_] = named_entities.get(ent.label_, [])\n",
        "        if ent.text.lower().strip():\n",
        "            named_entities[ent.label_].append(ent.text.lower())\n",
        "    return named_entities\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "\n",
        "\n",
        "def generate_passwords(named_entities, difficulty='easy', num_passwords=10):\n",
        "    special_chars = string.punctuation\n",
        "    possible_elements = []\n",
        "    for category, entities in named_entities.items():\n",
        "        possible_elements.extend([e for e in entities if e.strip()])\n",
        "\n",
        "    passwords = []\n",
        "    for _ in range(num_passwords):\n",
        "        if len(possible_elements) >= 3:\n",
        "            elements_to_use = random.sample(possible_elements, 3)\n",
        "            random.shuffle(elements_to_use)\n",
        "            password = \"\".join(elements_to_use)\n",
        "\n",
        "            if difficulty == 'medium':\n",
        "                password = password.capitalize()  # At least one uppercase\n",
        "                password += str(random.randint(0, 99))  # At least one digit\n",
        "\n",
        "            elif difficulty == 'difficult':\n",
        "                password = password.capitalize()  # At least one uppercase\n",
        "                password += random.choice(special_chars)  # At least one special character\n",
        "                password += str(random.randint(0, 99))  # At least one digit\n",
        "\n",
        "            passwords.append(password)\n",
        "        else:\n",
        "            passwords.append(\"Insufficient data\")\n",
        "\n",
        "    return passwords\n",
        "\n",
        "# List of Wikipedia URLs for notable people who died before 1980\n",
        "urls = [\n",
        "    'https://en.wikipedia.org/wiki/Albert_Einstein',\n",
        "    'https://en.wikipedia.org/wiki/Isaac_Newton',\n",
        "    'https://en.wikipedia.org/wiki/Galileo_Galilei',\n",
        "    # ... add more URLs\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Scrape Wikipedia\n",
        "profiles = get_profiles_from_urls(urls)\n",
        "\n",
        "\n",
        "# Step 2: Preprocess and Generate Passwords\n",
        "for name, profile_text in profiles.items():\n",
        "    print(\"----------------------------------------\")\n",
        "    print(f\"Generating possible passwords for {name}\")\n",
        "    cleaned_text = clean_text(profile_text)  # Clean the text first\n",
        "    preprocessed_text = preprocess_text(cleaned_text)  # Then preprocess\n",
        "    named_entities = extract_named_entities(preprocessed_text)\n",
        "    easy_passwords = generate_passwords(named_entities, difficulty='easy')\n",
        "    medium_passwords = generate_passwords(named_entities, difficulty='medium')\n",
        "    difficult_passwords = generate_passwords(named_entities, difficulty='difficult')\n",
        "\n",
        "    print(\"Easy Passwords:\")\n",
        "    for i, password in enumerate(easy_passwords, 1):\n",
        "        print(f\"{i}. {password}\")\n",
        "\n",
        "    print(\"\\nMedium Passwords:\")\n",
        "    for i, password in enumerate(medium_passwords, 1):\n",
        "        print(f\"{i}. {password}\")\n",
        "\n",
        "    print(\"\\nDifficult Passwords:\")\n",
        "    for i, password in enumerate(difficult_passwords, 1):\n",
        "        print(f\"{i}. {password}\")\n",
        "\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJTA6NLU39kX",
        "outputId": "37e7a90f-5e42-476f-84ad-80ad9afac0ed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Generating possible passwords for Albert Einstein\n",
            "Easy Passwords:\n",
            "1. united statesphd dissertation university zurichmarch april\n",
            "2. germanyear ageyear\n",
            "3. kaiser wilhelmgermanfirstly\n",
            "4. secondlyhumboldt universityyear\n",
            "5. theory relativityanazi warantan\n",
            "6. fourjewshumboldt university\n",
            "7. fourgermanbritish journal physics\n",
            "8. onefirst decades twentieth centurysecondly\n",
            "9. berlinbritish journal physicskaiser wilhelm\n",
            "10. fourberlinadolf hitler\n",
            "\n",
            "Medium Passwords:\n",
            "1. Humboldt universityswissantan80\n",
            "2. Theory relativityakaiser wilhelmgerman91\n",
            "3. Fourkaiser wilhelmtheory relativitya48\n",
            "4. World war iiberlingerman13\n",
            "5. Yeargermananstan64\n",
            "6. Twoyear ageswiss10\n",
            "7. Phd dissertation university zurichantanamerican27\n",
            "8. Switzerlandworld war iiswiss9\n",
            "9. Swissunited statesfirstly9\n",
            "10. United statesantanalbert einstein86\n",
            "\n",
            "Difficult Passwords:\n",
            "1. Germanyanstantheory relativitya$16\n",
            "2. Kaiser wilhelmtheory relativityagerman*87\n",
            "3. Adolf hitlergermanfour{5\n",
            "4. Berlingermanamerican&45\n",
            "5. British journal physicsalbert einsteinmarch april\"67\n",
            "6. Jewsgermanberlin=60\n",
            "7. Nazi waronetwo[86\n",
            "8. Yearjewsmarch april,99\n",
            "9. Germanjewsantan-86\n",
            "10. Twokaiser wilhelmfirst decades twentieth century{24\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Generating possible passwords for Isaac Newton\n",
            "Easy Passwords:\n",
            "1. principia mathematicadecember marchisaac newton\n",
            "2. firstprincipia mathematicaenglish\n",
            "3. newtonenglisheuropean\n",
            "4. onefirstlast three decades\n",
            "5. englishfirstsecond\n",
            "6. europeangermannewton\n",
            "7. centurieschristianfirst\n",
            "8. centuriesanne spentnewton\n",
            "9. december marchnewtonfirst\n",
            "10. twoonegerman\n",
            "\n",
            "Medium Passwords:\n",
            "1. Last three decadesanne spentnewton77\n",
            "2. Newtonanne spentparliament university cambridge knighted35\n",
            "3. Principia mathematicanewtonlast three decades46\n",
            "4. Anne spentlast three decadesfirst66\n",
            "5. Secondchristiantheologian35\n",
            "6. Europeanlondonfirst55\n",
            "7. Parliament university cambridge knighteddecember marchnewton fellow trinity college53\n",
            "8. Firstsecondnewton66\n",
            "9. Firsttheologiancenturies48\n",
            "10. Anne spentlondontheologian26\n",
            "\n",
            "Difficult Passwords:\n",
            "1. Firstanne spentfirst|29\n",
            "2. December marchanne spentlast three decades~7\n",
            "3. Secondnewtonprincipia mathematica%26\n",
            "4. December marchgermansecond,10\n",
            "5. Newton fellow trinity collegeanne spentdecember march-14\n",
            "6. Newtonfirstfirst.28\n",
            "7. Principia mathematicalast three decadestheologian`32\n",
            "8. Principia mathematicadecember marchnewton}49\n",
            "9. Firstfirstparliament university cambridge knighted<35\n",
            "10. Last three decadesparliament university cambridge knightedfirst~32\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Generating possible passwords for Galileo Galilei\n",
            "Easy Passwords:\n",
            "1. lleo lle galilayohcopernicandaily\n",
            "2. galileofouritalian\n",
            "3. galileocatholic churchlleo lle galilayoh\n",
            "4. february januarydailylleo lle galilayoh\n",
            "5. jupiteritalianlleo lle galilayoh\n",
            "6. italianfourlleo lle galilayoh\n",
            "7. dailycatholic churchgalileo\n",
            "8. galileoitalianfebruary january\n",
            "9. italianfebruary januaryjupiter\n",
            "10. february januarydailygalileo\n",
            "\n",
            "Medium Passwords:\n",
            "1. Jupitergalileogalileo60\n",
            "2. Lleo lle galilayohitalianfebruary january5\n",
            "3. Jupiterfourgalileo89\n",
            "4. February januaryitaliancopernican28\n",
            "5. Fourcatholic churchjupiter70\n",
            "6. Galileoitaliancopernican96\n",
            "7. February januarylleo lle galilayohgalileo33\n",
            "8. Italianfourlleo lle galilayoh42\n",
            "9. Italiangalileolleo lle galilayoh93\n",
            "10. Lleo lle galilayohjupitergalileo68\n",
            "\n",
            "Difficult Passwords:\n",
            "1. Dailyfourfebruary january-15\n",
            "2. Copernicanjupiteritalian?34\n",
            "3. Galileodailyjupiter@93\n",
            "4. Italiancatholic churchfour>8\n",
            "5. Fourcopernicanitalian~1\n",
            "6. Catholic churchgalileoitalian_98\n",
            "7. February januaryjupitercopernican!83\n",
            "8. Dailylleo lle galilayohitalian'67\n",
            "9. Galileoitaliancatholic church{26\n",
            "10. Dailygalileocopernican;92\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TG8SbuAA39iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IeHAyhiH547t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJ1Fpvsw545c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kZgtvlqZ543F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distilbert - easy & medium & difficult"
      ],
      "metadata": {
        "id": "85m561w25529"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZTG3klZ39fX",
        "outputId": "11cbe7a1-9a0c-4cf8-8dbc-f1804bc6bb4c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "\n",
        "# Initialize DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def get_entity_embedding(entity, tokenizer, model):\n",
        "    inputs = tokenizer(entity, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "def get_most_relevant_entities(named_entities, tokenizer, model):\n",
        "    main_subject_embedding = get_entity_embedding(\"Albert Einstein\", tokenizer, model)\n",
        "    relevant_entities = []\n",
        "    for category, entities in named_entities.items():\n",
        "        for entity in entities:\n",
        "            entity_embedding = get_entity_embedding(entity, tokenizer, model)\n",
        "            similarity = torch.cosine_similarity(main_subject_embedding, entity_embedding)\n",
        "            relevant_entities.append((entity, similarity))\n",
        "    relevant_entities = sorted(relevant_entities, key=lambda x: x[1], reverse=True)[:10]\n",
        "    return [entity for entity, similarity in relevant_entities]\n",
        "\n",
        "# ... (rest of the code for scraping and preprocessing)\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove numbers and special characters\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove citations like [1], [2], etc.\n",
        "    return text\n",
        "\n",
        "def get_profiles_from_urls(url_list):\n",
        "    profiles = {}\n",
        "    for url in url_list:\n",
        "        r = requests.get(url)\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        name = soup.select_one('.firstHeading').text\n",
        "        paragraphs = soup.select('.mw-parser-output p')\n",
        "        profile_text = ' '.join([p.text for p in paragraphs[:]])  # Taking the first 10 paragraphs for more context\n",
        "        profiles[name] = profile_text\n",
        "    return profiles\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token for token in doc if not token.is_punct and not token.is_space]\n",
        "    tokens = [token.text.lower() for token in tokens if token.text.lower() not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    named_entities = {}\n",
        "    for ent in doc.ents:\n",
        "        named_entities[ent.label_] = named_entities.get(ent.label_, [])\n",
        "        if ent.text.lower().strip():\n",
        "            named_entities[ent.label_].append(ent.text.lower())\n",
        "    return named_entities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 2: Preprocess, Get Relevant Entities, and Generate Passwords\n",
        "for name, profile_text in profiles.items():\n",
        "    print(\"----------------------------------------\")\n",
        "    print(f\"Generating possible passwords for {name}\")\n",
        "    cleaned_text = clean_text(profile_text)\n",
        "    preprocessed_text = preprocess_text(cleaned_text)\n",
        "    named_entities = extract_named_entities(preprocessed_text)\n",
        "\n",
        "    # Deduplication and sorting of entities\n",
        "    for key in named_entities:\n",
        "        named_entities[key] = list(set(named_entities[key]))\n",
        "\n",
        "    relevant_entities = get_most_relevant_entities(named_entities, tokenizer, model)\n",
        "\n",
        "    easy_passwords = generate_passwords({\"PERSON\": relevant_entities}, difficulty='easy')\n",
        "    medium_passwords = generate_passwords({\"PERSON\": relevant_entities}, difficulty='medium')\n",
        "    difficult_passwords = generate_passwords({\"PERSON\": relevant_entities}, difficulty='difficult')\n",
        "\n",
        "    print(\"Easy Passwords:\")\n",
        "    for i, password in enumerate(easy_passwords, 1):\n",
        "        print(f\"{i}. {password}\")\n",
        "\n",
        "    print(\"\\nMedium Passwords:\")\n",
        "    for i, password in enumerate(medium_passwords, 1):\n",
        "        print(f\"{i}. {password}\")\n",
        "\n",
        "    print(\"\\nDifficult Passwords:\")\n",
        "    for i, password in enumerate(difficult_passwords, 1):\n",
        "        print(f\"{i}. {password}\")\n",
        "\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80LAhqd-39dI",
        "outputId": "09fedf30-4f11-4d7c-966d-7ab61a87265e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Generating possible passwords for Albert Einstein\n",
            "Easy Passwords:\n",
            "1. british journal physicsprussian academy sciencesadolf hitler\n",
            "2. adolf hitlerbritish journal physicsfranklin roosevelt\n",
            "3. theory relativityajewsberlin\n",
            "4. theory relativityajewsfranklin roosevelt\n",
            "5. theory relativityahumboldt universitybritish journal physics\n",
            "6. franklin rooseveltberlinalbert einstein\n",
            "7. kaiser wilhelmalbert einsteinbritish journal physics\n",
            "8. jewstheory relativityahumboldt university\n",
            "9. humboldt universitytheory relativityaadolf hitler\n",
            "10. albert einsteinjewskaiser wilhelm\n",
            "\n",
            "Medium Passwords:\n",
            "1. Kaiser wilhelmtheory relativityajews57\n",
            "2. Albert einsteinberlintheory relativitya2\n",
            "3. British journal physicsberlintheory relativitya64\n",
            "4. Humboldt universityfranklin rooseveltadolf hitler95\n",
            "5. British journal physicskaiser wilhelmprussian academy sciences45\n",
            "6. Albert einsteinfranklin roosevelttheory relativitya62\n",
            "7. British journal physicskaiser wilhelmadolf hitler97\n",
            "8. Kaiser wilhelmbritish journal physicsadolf hitler73\n",
            "9. Humboldt universityalbert einsteinadolf hitler3\n",
            "10. Prussian academy sciencesfranklin rooseveltbritish journal physics78\n",
            "\n",
            "Difficult Passwords:\n",
            "1. Theory relativityafranklin rooseveltberlin+91\n",
            "2. Adolf hitlertheory relativityaalbert einstein$49\n",
            "3. Franklin rooseveltadolf hitlerberlin\"21\n",
            "4. Jewshumboldt universityadolf hitler'94\n",
            "5. Kaiser wilhelmfranklin rooseveltberlin=8\n",
            "6. Albert einsteinfranklin rooseveltbritish journal physics&80\n",
            "7. Theory relativityafranklin rooseveltberlin$20\n",
            "8. Kaiser wilhelmhumboldt universitytheory relativitya.99\n",
            "9. Adolf hitlerberlinalbert einstein!29\n",
            "10. British journal physicskaiser wilhelmjews>0\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Generating possible passwords for Isaac Newton\n",
            "Easy Passwords:\n",
            "1. centuriesnewtontheologian\n",
            "2. isaac newtonlondoncenturies\n",
            "3. newtongermanlondon\n",
            "4. londonenglishisaac newton\n",
            "5. newtonenglishisaac newton\n",
            "6. germanisaac newtonenglish\n",
            "7. twoenglishlondon\n",
            "8. newtonlondonenglish\n",
            "9. twocenturiestheologian\n",
            "10. englishsecondtwo\n",
            "\n",
            "Medium Passwords:\n",
            "1. Centurieslondonenglish42\n",
            "2. Newtontwolondon98\n",
            "3. Onegermannewton91\n",
            "4. Twoonegerman60\n",
            "5. Isaac newtonlondongerman4\n",
            "6. Secondenglishisaac newton93\n",
            "7. Twoisaac newtonone10\n",
            "8. Isaac newtonenglishtwo43\n",
            "9. Theologiantwocenturies48\n",
            "10. Theologiangermanisaac newton62\n",
            "\n",
            "Difficult Passwords:\n",
            "1. Londononeenglish*23\n",
            "2. Germancenturiesisaac newton@54\n",
            "3. Secondlondonisaac newton|91\n",
            "4. Newtoncenturiesgerman<22\n",
            "5. Twocenturiesenglish<67\n",
            "6. Newtonlondonsecond.19\n",
            "7. Twoisaac newtoncenturies,97\n",
            "8. Centuriesgermannewton]57\n",
            "9. Newtonlondonenglish`9\n",
            "10. Onenewtongerman\"50\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Generating possible passwords for Galileo Galilei\n",
            "Easy Passwords:\n",
            "1. lleo lle galilayohjupitercatholic church\n",
            "2. jupiteritaliandaily\n",
            "3. lleo lle galilayohitaliancopernican\n",
            "4. catholic churchcopernicandaily\n",
            "5. lleo lle galilayohfourcatholic church\n",
            "6. february januarylleo lle galilayohcatholic church\n",
            "7. galileocopernicanfebruary january\n",
            "8. copernicangalileofour\n",
            "9. catholic churchgalileofebruary january\n",
            "10. february januarycopernicandaily\n",
            "\n",
            "Medium Passwords:\n",
            "1. Galileoitaliandaily79\n",
            "2. Dailyfebruary januaryitalian55\n",
            "3. Italianlleo lle galilayohcopernican34\n",
            "4. February januarycopernicancatholic church40\n",
            "5. February januarydailylleo lle galilayoh9\n",
            "6. Dailyjupiterfour83\n",
            "7. February januaryitaliangalileo70\n",
            "8. Italiancopernicandaily71\n",
            "9. Catholic churchgalileocopernican32\n",
            "10. Lleo lle galilayohcatholic churchcopernican52\n",
            "\n",
            "Difficult Passwords:\n",
            "1. February januaryitalianjupiter&68\n",
            "2. Catholic churchdailyjupiter[24\n",
            "3. Jupitercopernicanlleo lle galilayoh!13\n",
            "4. Jupitercopernicangalileo>38\n",
            "5. Italiandailycopernican_87\n",
            "6. Dailycopernicanjupiter}31\n",
            "7. February januarygalileocatholic church{60\n",
            "8. Dailyfourcatholic church|84\n",
            "9. Galileocatholic churchfour^86\n",
            "10. Dailylleo lle galilayohitalian=51\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## refined version for Distilbert"
      ],
      "metadata": {
        "id": "_DojW2sn-vDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "\n",
        "# Initialize DistilBERT\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def get_entity_embedding(entity, tokenizer, model):\n",
        "    inputs = tokenizer(entity, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "def get_most_relevant_entities(named_entities, tokenizer, model):\n",
        "    main_subject_embedding = get_entity_embedding(\"Albert Einstein\", tokenizer, model)\n",
        "    relevant_entities = []\n",
        "    for category, entities in named_entities.items():\n",
        "        for entity in entities:\n",
        "            entity_embedding = get_entity_embedding(entity, tokenizer, model)\n",
        "            similarity = torch.cosine_similarity(main_subject_embedding, entity_embedding)\n",
        "            relevant_entities.append((entity, similarity))\n",
        "    relevant_entities = sorted(relevant_entities, key=lambda x: x[1], reverse=True)[:10]\n",
        "    return [entity for entity, similarity in relevant_entities]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fetch_profile(url):\n",
        "    try:\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()\n",
        "        soup = BeautifulSoup(r.content, 'html.parser')\n",
        "        return soup\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"An error occurred while fetching {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_name_and_text(soup):\n",
        "    name = soup.select_one('.firstHeading').text\n",
        "    paragraphs = soup.select('.mw-parser-output p')\n",
        "    profile_text = ' '.join([p.text for p in paragraphs[:10]])  # Extracting first 10 paragraphs\n",
        "    return name, profile_text\n",
        "\n",
        "def validate_password(password, difficulty):\n",
        "    if difficulty == 'medium':\n",
        "        return any(char.isupper() for char in password) and any(char.isdigit() for char in password)\n",
        "    elif difficulty == 'difficult':\n",
        "        return (any(char.isupper() for char in password) and\n",
        "                any(char.isdigit() for char in password) and\n",
        "                any(char in string.punctuation for char in password))\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    urls = [\n",
        "      'https://en.wikipedia.org/wiki/Albert_Einstein',\n",
        "      'https://en.wikipedia.org/wiki/Isaac_Newton',\n",
        "      'https://en.wikipedia.org/wiki/Galileo_Galilei',\n",
        "      # ... add more URLs\n",
        "  ]\n",
        "\n",
        "    profiles = {}\n",
        "    for url in urls:\n",
        "        soup = fetch_profile(url)\n",
        "        if soup:\n",
        "            name, profile_text = extract_name_and_text(soup)\n",
        "            profiles[name] = clean_text(profile_text)\n",
        "\n",
        "    for name, profile_text in profiles.items():\n",
        "        print(\"----------------------------------------\")\n",
        "        print(f\"Generating possible passwords for {name}\")\n",
        "\n",
        "        preprocessed_text = preprocess_text(profile_text)\n",
        "        named_entities = extract_named_entities(preprocessed_text)\n",
        "\n",
        "        # Deduplicate entities\n",
        "        for key in named_entities:\n",
        "            named_entities[key] = list(set(named_entities[key]))\n",
        "\n",
        "        relevant_entities = get_most_relevant_entities(named_entities, tokenizer, model)\n",
        "\n",
        "        for difficulty in ['easy', 'medium', 'difficult']:\n",
        "            passwords = generate_passwords({\"PERSON\": relevant_entities}, difficulty=difficulty)\n",
        "            valid_passwords = [pw for pw in passwords if validate_password(pw, difficulty)]\n",
        "\n",
        "            print(f\"\\n{difficulty.capitalize()} Passwords:\")\n",
        "            for i, password in enumerate(valid_passwords, 1):\n",
        "                print(f\"{i}. {password}\")\n",
        "\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-57LJ9tp-xDY",
        "outputId": "b41551c7-0614-4ac9-91b8-1016fbc2c448"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Generating possible passwords for Albert Einstein\n",
            "\n",
            "Easy Passwords:\n",
            "1. jewsfranklin roosevelthumboldt university\n",
            "2. berlinfranklin roosevelthumboldt university\n",
            "3. franklin rooseveltkaiser wilhelmalbert\n",
            "4. franklin rooseveltkaiser wilhelmalbert\n",
            "5. berlinalbert einsteinhumboldt university\n",
            "6. jewsalbert einsteinbritish journal physics\n",
            "7. jewsfranklin roosevelthumboldt university\n",
            "8. jewsbritish journal physicstheory relativitya\n",
            "9. adolf hitlerkaiser wilhelmfranklin roosevelt\n",
            "10. albertadolf hitlerberlin\n",
            "\n",
            "Medium Passwords:\n",
            "1. Franklin roosevelttheory relativityabritish journal physics58\n",
            "2. Franklin rooseveltadolf hitleralbert einstein17\n",
            "3. Franklin roosevelttheory relativityahumboldt university36\n",
            "4. Humboldt universitytheory relativityaadolf hitler26\n",
            "5. Berlinfranklin rooseveltbritish journal physics95\n",
            "6. Theory relativityaberlinhumboldt university49\n",
            "7. Albertfranklin rooseveltbritish journal physics1\n",
            "8. Jewskaiser wilhelmfranklin roosevelt17\n",
            "9. Franklin rooseveltberlintheory relativitya83\n",
            "10. Albertfranklin rooseveltbritish journal physics44\n",
            "\n",
            "Difficult Passwords:\n",
            "1. British journal physicstheory relativityakaiser wilhelm+93\n",
            "2. Franklin rooseveltkaiser wilhelmberlin@18\n",
            "3. Kaiser wilhelmalbert einsteinadolf hitler{58\n",
            "4. Albertkaiser wilhelmjews;10\n",
            "5. Theory relativityabritish journal physicsalbert einstein.66\n",
            "6. Kaiser wilhelmjewsalbert}79\n",
            "7. British journal physicsalbert einsteinhumboldt university,9\n",
            "8. Albert einsteinfranklin rooseveltjews-6\n",
            "9. Jewsberlinhumboldt university>67\n",
            "10. Albert einsteinjewshumboldt university.42\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Generating possible passwords for Isaac Newton\n",
            "\n",
            "Easy Passwords:\n",
            "1. newtonenglishlatin\n",
            "2. germanlatinhenry\n",
            "3. theologianlondonlatin\n",
            "4. greekaristotle newtonisaac newton\n",
            "5. germanhenryisaac newton\n",
            "6. englishisaac newtonhenry\n",
            "7. greeknewtonlondon\n",
            "8. englisharistotle newtonnewton\n",
            "9. theologianisaac newtonlatin\n",
            "10. theologiannewtonenglish\n",
            "\n",
            "Medium Passwords:\n",
            "1. Newtonlatinhenry88\n",
            "2. Greektheologianaristotle newton73\n",
            "3. Latingermangreek15\n",
            "4. Latintheologianlondon23\n",
            "5. Londonlatinenglish14\n",
            "6. Germanlondonhenry38\n",
            "7. Germannewtonisaac newton44\n",
            "8. Englisharistotle newtonnewton30\n",
            "9. Englishlatinnewton28\n",
            "10. Germantheologianenglish53\n",
            "\n",
            "Difficult Passwords:\n",
            "1. Latinaristotle newtongerman>54\n",
            "2. Aristotle newtonlatinisaac newton-62\n",
            "3. Aristotle newtonhenrytheologian&17\n",
            "4. Greekgermanlondon[37\n",
            "5. Greekhenrynewton^20\n",
            "6. Germanenglishlondon{39\n",
            "7. Isaac newtonenglishlatin^22\n",
            "8. Germanisaac newtonhenry<64\n",
            "9. Henryisaac newtonnewton[36\n",
            "10. Germanisaac newtonlatin`74\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Generating possible passwords for Galileo Galilei\n",
            "\n",
            "Easy Passwords:\n",
            "1. galileotwocatholic church\n",
            "2. threecatholic churchcentury\n",
            "3. galileomichelangelojupiter\n",
            "4. threecenturyitalian\n",
            "5. catholic churchthreeitalian\n",
            "6. catholic churchtwogalileo\n",
            "7. copernicantwomichelangelo\n",
            "8. galileocenturythree\n",
            "9. copernicancatholic churchitalian\n",
            "10. centuryitaliantwo\n",
            "\n",
            "Medium Passwords:\n",
            "1. Threemichelangelocentury21\n",
            "2. Copernicanthreegalileo78\n",
            "3. Italiancenturythree72\n",
            "4. Twocatholic churchgalileo4\n",
            "5. Jupitertwocentury35\n",
            "6. Galileogalileoitalian72\n",
            "7. Galileotwomichelangelo84\n",
            "8. Centurytwomichelangelo9\n",
            "9. Galileocatholic churchcopernican33\n",
            "10. Michelangelotwogalileo42\n",
            "\n",
            "Difficult Passwords:\n",
            "1. Michelangeloitalianjupiter/6\n",
            "2. Italiantwojupiter?63\n",
            "3. Threemichelangelotwo=62\n",
            "4. Catholic churchgalileocentury:22\n",
            "5. Italiantwocentury[49\n",
            "6. Italianjupitercatholic church~78\n",
            "7. Galileojupitermichelangelo{62\n",
            "8. Jupitercenturygalileo_91\n",
            "9. Copernicanthreejupiter]47\n",
            "10. Galileotwoitalian]13\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Using a given text"
      ],
      "metadata": {
        "id": "rNXzfydi-W7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import torch\n",
        "import spacy\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Initialize NLP and DistilBERT models\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean the text by removing special characters and citations.\"\"\"\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess the text using spaCy to remove stopwords and punctuation.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    \"\"\"Extract named entities using spaCy.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    named_entities = {}\n",
        "    for ent in doc.ents:\n",
        "        named_entities[ent.label_] = named_entities.get(ent.label_, [])\n",
        "        if ent.text.lower().strip():\n",
        "            named_entities[ent.label_].append(ent.text.lower())\n",
        "    return named_entities\n",
        "\n",
        "def get_entity_embedding(entity):\n",
        "    \"\"\"Get the DistilBERT embedding for a named entity.\"\"\"\n",
        "    inputs = tokenizer(entity, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "def get_most_relevant_entities(named_entities):\n",
        "    \"\"\"Get the most relevant entities based on DistilBERT embeddings.\"\"\"\n",
        "    relevant_entities = []\n",
        "    for category, entities in named_entities.items():\n",
        "        for entity in set(entities):  # Deduplicate entities\n",
        "            entity_embedding = get_entity_embedding(entity)\n",
        "            relevant_entities.append(entity)\n",
        "    return relevant_entities[:10]  # Limit to 10 most relevant entities\n",
        "\n",
        "def generate_passwords(relevant_entities, difficulty='easy', num_passwords=10):\n",
        "    \"\"\"Generate passwords based on relevant entities and difficulty level.\"\"\"\n",
        "    passwords = []\n",
        "    for _ in range(num_passwords):\n",
        "        elements = random.sample(relevant_entities, min(3, len(relevant_entities)))\n",
        "        random.shuffle(elements)\n",
        "        password = \"\".join(elements)\n",
        "\n",
        "        if difficulty == 'medium':\n",
        "            password = password.capitalize()\n",
        "            password += str(random.randint(0, 99))\n",
        "        elif difficulty == 'difficult':\n",
        "            password = password.capitalize()\n",
        "            password += random.choice(string.punctuation)\n",
        "            password += str(random.randint(0, 99))\n",
        "\n",
        "        passwords.append(password)\n",
        "    return passwords\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    profile_text = \"\"\"\n",
        "    Galileo Galilei was an Italian astronomer, physicist and engineer, sometimes described as a polymath.\n",
        "    He has been called the 'father of observational astronomy',\n",
        "    the 'father of modern physics', and the 'father of the scientific method'.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Generating possible passwords for Galileo Galilei\")\n",
        "\n",
        "    cleaned_text = clean_text(profile_text)\n",
        "    preprocessed_text = preprocess_text(cleaned_text)\n",
        "    named_entities = extract_named_entities(preprocessed_text)\n",
        "    relevant_entities = get_most_relevant_entities(named_entities)\n",
        "\n",
        "    for difficulty in ['easy', 'medium', 'difficult']:\n",
        "        passwords = generate_passwords(relevant_entities, difficulty)\n",
        "        print(f\"\\n{difficulty.capitalize()} Passwords:\")\n",
        "        for i, password in enumerate(passwords, 1):\n",
        "            print(f\"{i}. {password}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLsxtUN6-aeO",
        "outputId": "771aa35c-37d8-46c8-aaf2-f0d4992ef0d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating possible passwords for Galileo Galilei\n",
            "\n",
            "Easy Passwords:\n",
            "1. italian\n",
            "2. italian\n",
            "3. italian\n",
            "4. italian\n",
            "5. italian\n",
            "6. italian\n",
            "7. italian\n",
            "8. italian\n",
            "9. italian\n",
            "10. italian\n",
            "\n",
            "Medium Passwords:\n",
            "1. Italian83\n",
            "2. Italian78\n",
            "3. Italian43\n",
            "4. Italian8\n",
            "5. Italian31\n",
            "6. Italian43\n",
            "7. Italian58\n",
            "8. Italian68\n",
            "9. Italian29\n",
            "10. Italian72\n",
            "\n",
            "Difficult Passwords:\n",
            "1. Italian,53\n",
            "2. Italian^11\n",
            "3. Italian'23\n",
            "4. Italian@2\n",
            "5. Italian>75\n",
            "6. Italian/33\n",
            "7. Italian&35\n",
            "8. Italian+15\n",
            "9. Italian!90\n",
            "10. Italian&62\n"
          ]
        }
      ]
    }
  ]
}