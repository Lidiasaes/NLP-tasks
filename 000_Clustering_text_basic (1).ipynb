{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Content: **\n",
        "\n",
        "\n",
        "1.   Basic clustering\n",
        "2.   Basic clustering + POS tag and NER tags\n",
        "3.   Topic Modeling LDA + evaluating with Coherence\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kD4JxCeMkofc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPVG1qy4i094",
        "outputId": "f45bbfef-c30a-4f27-d3d1-9e2a0aec0690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) BASIC CLUSTERING - TEXTS**"
      ],
      "metadata": {
        "id": "SCILB7xpj0It"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxoG3mNBjfii",
        "outputId": "7b994dec-57af-4a2e-f701-bb075b1004f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data (replace with your dataset)\n",
        "documents = [\n",
        "    \"this is the first sentence\",\n",
        "    \"this sentence is similar to the first sentence\",\n",
        "    \"this is yet another sentence\",\n",
        "    \"this is a different kind of sentence\",\n",
        "    \"and this is a completely different topic\",\n",
        "    \"but this one is similar to the previous topic\",\n",
        "    \"and here is something unrelated\"\n",
        "]\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents)\n",
        "\n",
        "# KMeans Clustering (you can adjust the number of clusters as per your requirement)\n",
        "true_k = 2\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
        "model.fit(X)\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i in range(true_k):\n",
        "    print(\"Cluster %d:\" % i),\n",
        "    for ind in order_centroids[i, :10]:  # Printing top 10 terms per cluster\n",
        "        print(' %s' % terms[ind]),\n",
        "    print()\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Predictions for the documents:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"{doc} => Cluster {model.predict(vectorizer.transform([doc]))[0]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTey5OA9jc1h",
        "outputId": "3af543f4-6bbe-4765-d2e5-0507544c4b7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top terms per cluster:\n",
            "Cluster 0:\n",
            " topic\n",
            " unrelated\n",
            " something\n",
            " completely\n",
            " previous\n",
            " one\n",
            " different\n",
            " similar\n",
            " yet\n",
            " sentence\n",
            "\n",
            "Cluster 1:\n",
            " sentence\n",
            " first\n",
            " kind\n",
            " yet\n",
            " another\n",
            " different\n",
            " similar\n",
            " unrelated\n",
            " topic\n",
            " something\n",
            "\n",
            "\n",
            "\n",
            "Predictions for the documents:\n",
            "this is the first sentence => Cluster 1\n",
            "this sentence is similar to the first sentence => Cluster 1\n",
            "this is yet another sentence => Cluster 1\n",
            "this is a different kind of sentence => Cluster 1\n",
            "and this is a completely different topic => Cluster 0\n",
            "but this one is similar to the previous topic => Cluster 0\n",
            "and here is something unrelated => Cluster 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NTSC2Zxhj4h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ugtoUVVaj4gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 ) CLUSTERING - TEXTS + POS TAGs, NER TAGs**"
      ],
      "metadata": {
        "id": "VNQ4O6npj4wd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-IK-cgUjczR",
        "outputId": "b8bcc6d9-5ea4-4ca6-dc8b-a96aef94f159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tagger(doc):\n",
        "    return [token.pos_ for token in nlp(doc)]\n",
        "\n",
        "def ner_tagger(doc):\n",
        "    return [ent.label_ for ent in nlp(doc).ents]\n",
        "\n",
        "documents = [\n",
        "    \"this is the first sentence\",\n",
        "    \"Barack Obama was a president.\",\n",
        "    \"this is yet another sentence about New York\",\n",
        "    \"Apple is a different kind of company\",\n",
        "    \"and this is a completely different topic related to Microsoft\",\n",
        "    \"but this one is similar to the previous topic about Amazon\",\n",
        "    \"and here is something unrelated in 2022\"\n",
        "]\n",
        "\n",
        "# Extract features: POS tags and NER tags\n",
        "pos_features = [' '.join(pos_tagger(doc)) for doc in documents]\n",
        "ner_features = [' '.join(ner_tagger(doc)) for doc in documents]\n",
        "\n",
        "# Vectorize using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
        "\n",
        "combined_features = FeatureUnion([\n",
        "    ('text', tfidf_vectorizer),\n",
        "    ('pos', TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=False)),\n",
        "    ('ner', TfidfVectorizer(tokenizer=lambda x: x.split(), lowercase=False))\n",
        "])\n",
        "\n",
        "X = combined_features.fit_transform(documents + pos_features + ner_features)\n",
        "\n",
        "# KMeans Clustering\n",
        "true_k = 3\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
        "model.fit(X)\n",
        "\n",
        "# Display results\n",
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = combined_features.get_feature_names_out()\n",
        "for i in range(true_k):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    for ind in order_centroids[i, :10]:  # top 10 terms per cluster\n",
        "        print(f\" {terms[ind]}\")\n",
        "    print()\n",
        "\n",
        "print(\"\\nPredictions for the documents:\")\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"{doc} => Cluster {model.predict(combined_features.transform([doc]))[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbo0faI2kBSF",
        "outputId": "90210146-614e-4fad-8296-e3361d92fa32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top terms per cluster:\n",
            "Cluster 0:\n",
            " pos__is\n",
            " ner__is\n",
            " pos__this\n",
            " ner__this\n",
            " text__sentence\n",
            " ner__DATE\n",
            " text__person\n",
            " pos__ORDINAL\n",
            " pos__GPE\n",
            " pos__DATE\n",
            "\n",
            "Cluster 1:\n",
            " ner__NOUN\n",
            " pos__NOUN\n",
            " text__noun\n",
            " pos__PROPN\n",
            " text__propn\n",
            " ner__PROPN\n",
            " ner__DET\n",
            " pos__DET\n",
            " text__det\n",
            " text__aux\n",
            "\n",
            "Cluster 2:\n",
            " ner__ORG\n",
            " pos__ORG\n",
            " text__org\n",
            " ner__yet\n",
            " pos__AUX\n",
            " pos__DATE\n",
            " pos__CCONJ\n",
            " pos__Barack\n",
            " pos__Apple\n",
            " pos__Amazon\n",
            "\n",
            "\n",
            "Predictions for the documents:\n",
            "this is the first sentence => Cluster 0\n",
            "Barack Obama was a president. => Cluster 0\n",
            "this is yet another sentence about New York => Cluster 0\n",
            "Apple is a different kind of company => Cluster 0\n",
            "and this is a completely different topic related to Microsoft => Cluster 0\n",
            "but this one is similar to the previous topic about Amazon => Cluster 0\n",
            "and here is something unrelated in 2022 => Cluster 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZxnPPVaGkftB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kbS7dpckfqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) LDA - TOPIC MODELING**"
      ],
      "metadata": {
        "id": "RK1bMEPdkgGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yj1UlH0mkl5C",
        "outputId": "dc42e415-ad7d-4b6e-f698-c019b1ee492d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiOOAbK-mRzu",
        "outputId": "d561ec42-a7d7-4933-dc0e-76c0b8ba0543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.24.2 (from pyLDAvis)\n",
            "  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.2)\n",
            "Collecting pandas>=2.0.0 (from pyLDAvis)\n",
            "  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.8.5)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas>=2.0.0->pyLDAvis)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: funcy, tzdata, numpy, pandas, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 numpy-1.26.0 pandas-2.1.1 pyLDAvis-3.4.1 tzdata-2023.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# load and preprocess data\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-QfoZ6glFTa",
        "outputId": "e81ab8c8-a76c-46a1-8a1b-277ced4e7866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and Preprocess the Data\n",
        "newsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
        "documents = newsgroups.data\n",
        "\n",
        "# Preprocess the data\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(doc):\n",
        "    tokens = tokenizer.tokenize(doc.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 2]\n",
        "    return tokens\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSmNXpxTlW1P",
        "outputId": "937b7829-855e-4984-b037-d84bf2d71dfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Prepare Data for LDA\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9EHXr2gmbCw",
        "outputId": "0ac417c6-2892-4b82-c41e-aa12330b467d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 3. Apply LDA\n",
        "\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Define a range of topics you want to explore\n",
        "topic_range = list(range(3, 21, 5))  #  from 3 to 20 topics, with a step of 5.\n",
        "\n",
        "# Lists to store the results\n",
        "models = []\n",
        "coherences = []\n",
        "\n",
        "for num_topics in topic_range:\n",
        "    # Apply LDA\n",
        "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
        "    models.append(lda_model)\n",
        "\n",
        "    # Evaluate Using Coherence\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence = coherence_model.get_coherence()\n",
        "    coherences.append(coherence)\n",
        "\n",
        "# Print the coherence scores\n",
        "for num_topics, coherence in zip(topic_range, coherences):\n",
        "    print(f\"Number of Topics: {num_topics}, Coherence Score: {coherence:.4f}\")\n",
        "\n",
        "# Based on the printed scores, choose a model\n",
        "chosen_model_index = coherences.index(max(coherences))\n",
        "chosen_model = models[chosen_model_index]\n",
        "chosen_num_topics = topic_range[chosen_model_index]\n",
        "\n",
        "print(f\"\\nBest Model has {chosen_num_topics} topics with a coherence score of {max(coherences):.4f}\")\n",
        "\n",
        "# Inspect Topics for the chosen model\n",
        "topics = chosen_model.print_topics(num_words=10)\n",
        "for topic in topics:\n",
        "    print(topic)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iqeEWNSmedP",
        "outputId": "39df063e-d8fe-4275-abbb-6504d96012f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Topics: 3, Coherence Score: 0.6001\n",
            "Number of Topics: 8, Coherence Score: 0.6567\n",
            "Number of Topics: 13, Coherence Score: 0.6551\n",
            "Number of Topics: 18, Coherence Score: 0.6286\n",
            "\n",
            "Best Model has 8 topics with a coherence score of 0.6567\n",
            "(0, '0.088*\"max\" + 0.025*\"g9v\" + 0.017*\"b8f\" + 0.014*\"a86\" + 0.011*\"1d9\" + 0.011*\"145\" + 0.009*\"34u\" + 0.008*\"bhj\" + 0.007*\"giz\" + 0.007*\"75u\"')\n",
            "(1, '0.008*\"window\" + 0.008*\"drive\" + 0.007*\"system\" + 0.006*\"one\" + 0.006*\"use\" + 0.006*\"problem\" + 0.006*\"would\" + 0.006*\"thanks\" + 0.006*\"card\" + 0.006*\"know\"')\n",
            "(2, '0.008*\"government\" + 0.007*\"state\" + 0.006*\"gun\" + 0.006*\"law\" + 0.005*\"right\" + 0.004*\"people\" + 0.004*\"armenian\" + 0.004*\"year\" + 0.004*\"president\" + 0.004*\"american\"')\n",
            "(3, '0.010*\"edu\" + 0.010*\"space\" + 0.005*\"com\" + 0.004*\"nasa\" + 0.004*\"new\" + 0.003*\"university\" + 0.003*\"1993\" + 0.003*\"center\" + 0.003*\"satellite\" + 0.003*\"launch\"')\n",
            "(4, '0.010*\"one\" + 0.010*\"would\" + 0.009*\"people\" + 0.007*\"god\" + 0.006*\"know\" + 0.006*\"think\" + 0.006*\"say\" + 0.005*\"like\" + 0.005*\"time\" + 0.004*\"thing\"')\n",
            "(5, '0.008*\"game\" + 0.008*\"year\" + 0.007*\"one\" + 0.007*\"get\" + 0.007*\"car\" + 0.006*\"would\" + 0.006*\"like\" + 0.006*\"team\" + 0.006*\"good\" + 0.005*\"time\"')\n",
            "(6, '0.013*\"key\" + 0.012*\"file\" + 0.007*\"use\" + 0.006*\"program\" + 0.006*\"entry\" + 0.005*\"available\" + 0.005*\"information\" + 0.005*\"system\" + 0.005*\"server\" + 0.005*\"edu\"')\n",
            "(7, '0.003*\"chz\" + 0.003*\"sc_\" + 0.002*\"scx\" + 0.002*\"x_s\" + 0.002*\"13p\" + 0.002*\"8c_\" + 0.002*\"lhz\" + 0.002*\"w47\" + 0.001*\"13q\" + 0.001*\"cj1\"')\n"
          ]
        }
      ]
    }
  ]
}