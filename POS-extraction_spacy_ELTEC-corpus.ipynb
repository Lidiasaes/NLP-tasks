{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9YAgkI8Naco"
      },
      "source": [
        "# Extracting part of speech from ELTeC-ENG\n",
        "\n",
        "Adaptation of a great Colab by Borja Navarro for the LT4DH course in the University of the Basque Country.\n",
        "\n",
        "This version (to be cleaned) uses English resources in contrast to the Spanish one used by Borja Navarro.\n",
        "\n",
        "Original data here:\n",
        "\n",
        "Borja Navarro Colorado | University of Alicante\n",
        "\n",
        "In this case, the information about part of speech has not been manually annotated in the corpus. It is necessary first analyze the novels with a NLP system and then extract the linguistic information. The NLP system used is [SpaCy](https://spacy.io/).\n",
        "\n",
        "The notebook shows:\n",
        "\n",
        "- how to open a novel from ELTeC in COLAB and to analyse it with SpaCy, and\n",
        "- analysing the output of Spacy for DH.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2j6tERTNTcP"
      },
      "source": [
        "## Loading ELTeC-SPA corpus in Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kejn4RdANZEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57c42af-1cbe-40c5-aacd-f6243cf5d6c3"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!wget \"https://github.com/COST-ELTeC/ELTeC-eng/archive/refs/heads/master.zip\" # paste here corpus url\n",
        "\n",
        "zip_ref = zipfile.ZipFile('master.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall() #Extracts files here (/content/)\n",
        "zip_ref.close() \n",
        "!rm master.zip #Removes ZIP to save space"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-16 12:59:39--  https://github.com/COST-ELTeC/ELTeC-eng/archive/refs/heads/master.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/COST-ELTeC/ELTeC-eng/zip/refs/heads/master [following]\n",
            "--2023-04-16 12:59:39--  https://codeload.github.com/COST-ELTeC/ELTeC-eng/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 20.27.177.114\n",
            "Connecting to codeload.github.com (codeload.github.com)|20.27.177.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘master.zip’\n",
            "\n",
            "master.zip              [        <=>         ]  91.32M  3.95MB/s    in 22s     \n",
            "\n",
            "2023-04-16 13:00:02 (4.06 MB/s) - ‘master.zip’ saved [95756782]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLkW02KoOIX6"
      },
      "source": [
        "## SpaCy: download and installing\n",
        "\n",
        "[SpaCy](https://spacy.io/) is a NLP system. It analyzes part of speech and lemmas, sintax (dependencies) and named entities. \n",
        "\n",
        "Three steps:\n",
        "\n",
        "1. Import SpaCy to Colab\n",
        "2. Download langauge module (Spanish)\n",
        "3. Activate module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNHMbKkERXhI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd6e6c32-33bd-4c52-9472-1cc8c3b44da0"
      },
      "source": [
        "import spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm #Download English module (the \"small\" module in this case: \"sm\").\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp_eng = en_core_web_sm.load() #Load English analyzer in \"nlp_eng\"."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-16 13:00:11.829982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsk-hIbqRuw5"
      },
      "source": [
        "## Analyzing a novel from ELTeC-SPA\n",
        "\n",
        "Once we have downloaded the corpus and activated SpaCy, let's analyze one novel.\n",
        "\n",
        "First, select from the corpus [ELTeC-SPA](https://github.com/COST-ELTeC/ELTeC-spa/tree/master/level1) a novel and copy the file name. Then paste the name in the variable \"novela_name\". In this example, we will analyze the novel of Gertrudis Gómez de Avellaneda [*Sab*](https://github.com/COST-ELTeC/ELTeC-spa/blob/master/level1/SPA1021_GomezDeAvellaneda_Sab.xml): SPA1021_GomezDeAvellaneda_Sab.xml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVJtGl9vRzpq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf67037-5669-456d-eed1-9909c84bf5e7"
      },
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "novela_name = \"ENG18621_Braddon.xml\" # T1: ENG18471_Bronte.xml      //  T2:  ENG18621_Braddon.xml         Put here the name of the file\n",
        "dir_in = \"/content/ELTeC-eng-master/level1/\"\n",
        "\n",
        "novela_text = '' \n",
        "\n",
        "print('Analyzing', novela_name)\n",
        "\n",
        "ficheroEntrada = dir_in + novela_name\n",
        "with open(ficheroEntrada, 'r') as tei: #Opens the file\n",
        "  print(\"Opening the file and extracting text\")\n",
        "  soup = BeautifulSoup(tei, 'xml') #Parse the XML\n",
        "  capitulos = soup.find_all(type=\"chapter\") #Only chapters are taking into account. No letters (To Do)\n",
        "  for cap in capitulos:\n",
        "    parrafos = cap.find_all('p') #Extract all paragraphs of each chapter\n",
        "    for parrafo in parrafos:\n",
        "      #print(parrafo.text)\n",
        "      novela_text+=parrafo.text+'\\n'\n",
        "\n",
        "print('Analyzing PoS and lemmas')\n",
        "analisis = nlp_eng(novela_text) #Here the novel is analyzed with SpaCy. All the analysis is stored in \"analisis\" variable.\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing ENG18471_Bronte.xml\n",
            "Opening the file and extracting text\n",
            "Analyzing PoS and lemmas\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_13VKGg9A7Z"
      },
      "source": [
        "Now all the analysis is stored in \"analisis\" variable. It only remains to iterate over the variable and extract the information: in this case, part of speech. How to extract information about syntax, named entities, etc. see [SpaCy 101](https://spacy.io/usage/spacy-101)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7Zqf9ZC6-0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528b2ad7-1dcd-48ea-b135-c349d88f4c18"
      },
      "source": [
        "NVA = '\\tNovel\\tNouns\\tVerbs\\tAdjectives\\tUnique_nouns\\tUnique_verbs\\tUnique_adjs\\n' #\n",
        "\n",
        "nom_novela = 'The Daughters of Danaus'\n",
        "#nom_novela = novela_name\n",
        "\n",
        "nouns=[]\n",
        "verbs=[]\n",
        "adjs=[]\n",
        "\n",
        "noun_counts= dict()\n",
        "verb_counts= dict()\n",
        "adj_counts= dict()\n",
        "\n",
        "# for token in analisis: \n",
        "#   if token.pos_ == 'NOUN':\n",
        "#     if token.text.lower() in noun_counts:\n",
        "#        noun_counts[token.text.lower()] += 1\n",
        "#     else:\n",
        "#        noun_counts[token.text.lower()] = 1\n",
        "#   elif token.pos_ == 'VERB':\n",
        "#     if token.text.lower() in verb_counts:\n",
        "#        verb_counts[token.text.lower()] += 1\n",
        "#     else:\n",
        "#         verb_counts[token.text.lower()] = 1\n",
        "#   elif token.pos_ == 'ADJ':\n",
        "#     if token.text.lower() in adj_counts:\n",
        "#        adj_counts[token.text.lower()] += 1\n",
        "#     else:\n",
        "#        adj_counts[token.text.lower()] = 1\n",
        "\n",
        "for token in analisis: \n",
        "  if token.pos_ == 'NOUN':\n",
        "    if token.lemma_ in noun_counts:\n",
        "       noun_counts[token.lemma_] += 1\n",
        "    else:\n",
        "       noun_counts[token.lemma_] = 1\n",
        "  elif token.pos_ == 'VERB':\n",
        "    if token.lemma_ in verb_counts:\n",
        "       verb_counts[token.lemma_] += 1\n",
        "    else:\n",
        "        verb_counts[token.lemma_] = 1\n",
        "  elif token.pos_ == 'ADJ':\n",
        "    if token.lemma_ in adj_counts:\n",
        "       adj_counts[token.lemma_] += 1\n",
        "    else:\n",
        "       adj_counts[token.lemma_] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Sort the noun_counts dictionary by appearance count in descending order\n",
        "sorted_nouns = sorted(noun_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_verbs = sorted(verb_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "sorted_adjs = sorted(adj_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sorted nouns and their appearance counts\n",
        "for i, (noun, count) in enumerate(sorted_nouns):\n",
        "    if i >= 50:\n",
        "        break\n",
        "    print(noun, count)\n",
        "\n",
        "print(\"\\n-----------------\\n\")\n",
        "\n",
        "for i, (verb, count) in enumerate(sorted_verbs):\n",
        "    if i >= 50:\n",
        "        break\n",
        "    print(verb, count)\n",
        "\n",
        "print(\"\\n-----------------\\n\")\n",
        "\n",
        "for i, (adj, count) in enumerate(sorted_adjs):\n",
        "    if i >= 50:\n",
        "        break\n",
        "    print(adj, count)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "master 183\n",
            "door 161\n",
            "hand 150\n",
            "time 148\n",
            "house 143\n",
            "eye 140\n",
            "day 139\n",
            "father 126\n",
            "face 111\n",
            "night 111\n",
            "room 107\n",
            "man 103\n",
            "head 103\n",
            "thing 96\n",
            "way 93\n",
            "word 92\n",
            "hour 88\n",
            "child 87\n",
            "heart 83\n",
            "place 80\n",
            "fire 80\n",
            "bed 76\n",
            "window 74\n",
            "evening 70\n",
            "lady 68\n",
            "minute 68\n",
            "cousin 67\n",
            "kitchen 66\n",
            "servant 65\n",
            "side 64\n",
            "arm 63\n",
            "morning 62\n",
            "book 61\n",
            "one 59\n",
            "year 59\n",
            "life 58\n",
            "mind 57\n",
            "stair 57\n",
            "home 55\n",
            "chair 53\n",
            "friend 52\n",
            "mistress 50\n",
            "death 49\n",
            "companion 47\n",
            "table 45\n",
            "road 45\n",
            "boy 44\n",
            "world 44\n",
            "grange 44\n",
            "dog 42\n",
            "\n",
            "-----------------\n",
            "\n",
            "have 560\n",
            "say 533\n",
            "go 407\n",
            "come 357\n",
            "do 314\n",
            "see 312\n",
            "take 258\n",
            "make 239\n",
            "think 237\n",
            "tell 213\n",
            "get 209\n",
            "look 189\n",
            "know 184\n",
            "answer 180\n",
            "hear 161\n",
            "cry 157\n",
            "leave 152\n",
            "ask 147\n",
            "let 143\n",
            "be 140\n",
            "give 139\n",
            "sit 124\n",
            "reply 121\n",
            "keep 119\n",
            "speak 119\n",
            "wish 113\n",
            "feel 107\n",
            "turn 104\n",
            "put 100\n",
            "return 96\n",
            "begin 93\n",
            "seem 91\n",
            "bring 90\n",
            "want 89\n",
            "call 88\n",
            "find 84\n",
            "love 83\n",
            "stand 83\n",
            "exclaim 82\n",
            "enter 80\n",
            "continue 78\n",
            "run 77\n",
            "live 73\n",
            "lie 72\n",
            "hold 69\n",
            "walk 67\n",
            "grow 67\n",
            "bear 67\n",
            "show 66\n",
            "set 65\n",
            "\n",
            "-----------------\n",
            "\n",
            "little 174\n",
            "good 124\n",
            "other 113\n",
            "young 113\n",
            "own 107\n",
            "more 100\n",
            "last 94\n",
            "bad 91\n",
            "old 85\n",
            "great 77\n",
            "first 62\n",
            "whole 55\n",
            "poor 49\n",
            "same 48\n",
            "such 47\n",
            "ill 46\n",
            "sure 46\n",
            "well 43\n",
            "next 43\n",
            "few 42\n",
            "long 40\n",
            "black 39\n",
            "least 39\n",
            "much 39\n",
            "dead 39\n",
            "full 35\n",
            "happy 35\n",
            "cold 32\n",
            "glad 32\n",
            "certain 31\n",
            "strange 30\n",
            "strong 29\n",
            "afraid 29\n",
            "open 29\n",
            "many 28\n",
            "ready 28\n",
            "sorry 27\n",
            "angry 27\n",
            "new 26\n",
            "right 26\n",
            "present 26\n",
            "former 26\n",
            "wicked 26\n",
            "wild 26\n",
            "white 25\n",
            "quiet 25\n",
            "short 24\n",
            "deep 24\n",
            "fine 24\n",
            "only 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nlp_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "novela_name = \"ENG18621_Braddon.xml\" # Put here the name of the file\n",
        "dir_in = \"/content/ELTeC-eng-master/level1/\"\n",
        "\n",
        "novela_text = '' \n",
        "\n",
        "print('Analyzing', novela_name)\n",
        "\n",
        "ficheroEntrada = dir_in + novela_name\n",
        "with open(ficheroEntrada, 'r') as tei: #Opens the file\n",
        "  print(\"Opening the file and extracting text\")\n",
        "  soup = BeautifulSoup(tei, 'xml') #Parse the XML\n",
        "  capitulos = soup.find_all(type=\"chapter\") #Only chapters are taking into account. No letters (To Do)\n",
        "  for cap in capitulos:\n",
        "    parrafos = cap.find_all('p') #Extract all paragraphs of each chapter\n",
        "    for parrafo in parrafos:\n",
        "      #print(parrafo.text)\n",
        "      novela_text+=parrafo.text+'\\n'\n",
        "\n",
        "print('Done reading the novel text')\n",
        "\n",
        "palabra_busqueda = input('Introduce la palabra que deseas buscar: ')\n",
        "doc = nlp_eng(novela_text)\n",
        "for token in doc:\n",
        "  if token.text.lower() == palabra_busqueda.lower():\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJrwC2kpFQDN",
        "outputId": "7b55bad5-67e9-4376-803d-dd7383ebc5a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing ENG18471_Bronte.xml\n",
            "Opening the file and extracting text\n",
            "Done reading the novel text\n",
            "Introduce la palabra que deseas buscar: domestic\n",
            "domestic ADJ\n",
            "domestic ADJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nlp_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "novela_name = \"ENG18621_Braddon.xml\" # Put here the name of the file\n",
        "dir_in = \"/content/ELTeC-eng-master/level1/\"\n",
        "\n",
        "novela_text = '' \n",
        "\n",
        "print('Analyzing', novela_name)\n",
        "\n",
        "ficheroEntrada = dir_in + novela_name\n",
        "with open(ficheroEntrada, 'r') as tei: #Opens the file\n",
        "  print(\"Opening the file and extracting text\")\n",
        "  soup = BeautifulSoup(tei, 'xml') #Parse the XML\n",
        "  capitulos = soup.find_all(type=\"chapter\") #Only chapters are taking into account. No letters (To Do)\n",
        "  for cap in capitulos:\n",
        "    parrafos = cap.find_all('p') #Extract all paragraphs of each chapter\n",
        "    for parrafo in parrafos:\n",
        "      #print(parrafo.text)\n",
        "      novela_text+=parrafo.text+'\\n'\n",
        "\n",
        "print('Done reading the novel text')\n",
        "\n",
        "palabra_busqueda = input('Introduce la palabra que deseas buscar: ')\n",
        "doc = nlp_eng(novela_text)\n",
        "\n",
        "pos_count = {} # Diccionario para almacenar el conteo de las etiquetas POS\n",
        "\n",
        "for token in doc:\n",
        "  if token.text.lower() == palabra_busqueda.lower():\n",
        "    if token.pos_ in pos_count:\n",
        "      pos_count[token.pos_] += 1 # Incrementa el conteo si la etiqueta ya existe en el diccionario\n",
        "    else:\n",
        "      pos_count[token.pos_] = 1 # Agrega la etiqueta al diccionario si es la primera vez que aparece\n",
        "\n",
        "if len(pos_count) == 0:\n",
        "  print('No se encontró la palabra', palabra_busqueda)\n",
        "else:\n",
        "  print('Conteo de etiquetas POS para', palabra_busqueda)\n",
        "  for pos in pos_count:\n",
        "    print(pos, ':', pos_count[pos])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxxsgzJjF79J",
        "outputId": "75713f60-09e6-4b6d-eaf2-cba102f68881"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing ENG18471_Bronte.xml\n",
            "Opening the file and extracting text\n",
            "Done reading the novel text\n",
            "Introduce la palabra que deseas buscar: domestic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nlp_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "novela_name = \"ENG18621_Braddon.xml\" # Put here the name of the file\n",
        "dir_in = \"/content/ELTeC-eng-master/level1/\"\n",
        "\n",
        "novela_text = '' \n",
        "\n",
        "print('Analyzing', novela_name)\n",
        "\n",
        "ficheroEntrada = dir_in + novela_name\n",
        "with open(ficheroEntrada, 'r') as tei: #Opens the file\n",
        "  print(\"Opening the file and extracting text\")\n",
        "  soup = BeautifulSoup(tei, 'xml') #Parse the XML\n",
        "  capitulos = soup.find_all(type=\"chapter\") #Only chapters are taking into account. No letters (To Do)\n",
        "  for cap in capitulos:\n",
        "    parrafos = cap.find_all('p') #Extract all paragraphs of each chapter\n",
        "    for parrafo in parrafos:\n",
        "      #print(parrafo.text)\n",
        "      novela_text+=parrafo.text+'\\n'\n",
        "\n",
        "print('Done reading the novel text')\n",
        "\n",
        "palabra_busqueda = input('Introduce la palabra que deseas buscar: ')\n",
        "doc = nlp_eng(novela_text)\n",
        "\n",
        "pos_count = {} # Diccionario para almacenar el conteo de las etiquetas POS\n",
        "sentences_with_word = [] # Lista para almacenar las frases en las que aparece la palabra buscada\n",
        "\n",
        "for sent in doc.sents:\n",
        "  if palabra_busqueda.lower() in sent.text.lower():\n",
        "    for token in sent:\n",
        "      if token.text.lower() == palabra_busqueda.lower():\n",
        "        if token.pos_ in pos_count:\n",
        "          pos_count[token.pos_] += 1 # Incrementa el conteo si la etiqueta ya existe en el diccionario\n",
        "        else:\n",
        "          pos_count[token.pos_] = 1 # Agrega la etiqueta al diccionario si es la primera vez que aparece\n",
        "    sentences_with_word.append(sent.text.strip())\n",
        "\n",
        "if len(pos_count) == 0:\n",
        "  print('No se encontró la palabra', palabra_busqueda)\n",
        "else:\n",
        "  print('Conteo de etiquetas POS para', palabra_busqueda)\n",
        "  for pos in pos_count:\n",
        "    print(pos, ':', pos_count[pos])\n",
        "  print('\\nFrases en las que aparece', palabra_busqueda)\n",
        "  for sentence in sentences_with_word:\n",
        "    print(sentence)\n"
      ],
      "metadata": {
        "id": "rd1Tp-0wGimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "nlp_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "novela_name = \"ENG18621_Braddon.xml\" # Put here the name of the file\n",
        "dir_in = \"/content/ELTeC-eng-master/level1/\"\n",
        "\n",
        "novela_text = '' \n",
        "\n",
        "print('Analyzing', novela_name)\n",
        "\n",
        "ficheroEntrada = dir_in + novela_name\n",
        "with open(ficheroEntrada, 'r') as tei: #Opens the file\n",
        "  print(\"Opening the file and extracting text\")\n",
        "  soup = BeautifulSoup(tei, 'xml') #Parse the XML\n",
        "  capitulos = soup.find_all(type=\"chapter\") #Only chapters are taking into account. No letters (To Do)\n",
        "  for cap in capitulos:\n",
        "    parrafos = cap.find_all('p') #Extract all paragraphs of each chapter\n",
        "    for parrafo in parrafos:\n",
        "      #print(parrafo.text)\n",
        "      novela_text+=parrafo.text+'\\n'\n",
        "\n",
        "print('Done reading the novel text')\n",
        "\n",
        "palabra_busqueda = input('Introduce la palabra que deseas buscar: ')\n",
        "doc = nlp_eng(novela_text)\n",
        "\n",
        "pos_count = {} # Diccionario para almacenar el conteo de las etiquetas POS\n",
        "sentences_with_word = [] # Lista para almacenar las frases en las que aparece la palabra buscada\n",
        "\n",
        "for sent in doc.sents:\n",
        "  if palabra_busqueda.lower() in sent.text.lower():\n",
        "    pos_dict = {} # Diccionario para almacenar la etiqueta POS de la palabra buscada en la frase actual\n",
        "    for token in sent:\n",
        "      if token.text.lower() == palabra_busqueda.lower():\n",
        "        if token.pos_ in pos_count:\n",
        "          pos_count[token.pos_] += 1 # Incrementa el conteo si la etiqueta ya existe en el diccionario\n",
        "        else:\n",
        "          pos_count[token.pos_] = 1 # Agrega la etiqueta al diccionario si es la primera vez que aparece\n",
        "        pos_dict[token.text] = token.pos_ # Agrega la etiqueta POS de la palabra buscada al diccionario pos_dict\n",
        "    sentences_with_word.append((sent.text.strip(), pos_dict)) # Agrega la frase actual junto con su diccionario pos_dict a la lista sentences_with_word\n",
        "\n",
        "if len(pos_count) == 0:\n",
        "  print('No se encontró la palabra', palabra_busqueda)\n",
        "else:\n",
        "  print('Conteo de etiquetas POS para', palabra_busqueda)\n",
        "  for pos in pos_count:\n",
        "    print(pos, ':', pos_count[pos])\n",
        "  print('\\nFrases en las que aparece', palabra_busqueda)\n",
        "  for sentence, pos_dict in sentences_with_word:\n",
        "    print(sentence)\n",
        "    for word, pos in pos_dict.items():\n",
        "      print(f\"  '{word}' -> {pos}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "CVvWer-SHGAn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
