
## Tutorial: https://www.youtube.com/watch?v=uZxQz87lb84&t=199s

""" 
pip install bertopic
pip install bertopic[flair]
pip install bertopic[gensim] 
pip install bertopic[spacy]
pip install bertopic[use]
pip install bertopic[vision]
pip install umap-learn==0.5.1
pip install BERTopic==0.7.0


"""

# @numba.jit(nopython=True)


from bertopic import BERTopic
import json
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
# saco data de una librería
docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']

# puedo poner language = "multilingual"
topic_model = BERTopic(language="english", calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(docs)


## EXTRAER TOPICS -- the most frequent topic, the one reflecting the best the rest 
freq = topic_model.get_topic_info(); freq.head(5)
print(freq) # si sale -1 significa que es outlier, es decir, it is not significant

print(topic_model.get_topic(0))  # select the most frequent topic


"""
Different attributes one can use 

topics_	The topics that are generated for each document after training or updating the topic model.
probabilities_	The probabilities that are generated for each document if HDBSCAN is used.
topic_sizes_	The size of each topic
topic_mapper_	A class for tracking topics and their mappings anytime they are merged/reduced.
topic_representations_	The top n terms per topic and their respective c-TF-IDF values.
c_tf_idf_	The topic-term matrix as calculated through c-TF-IDF.
topic_labels_	The default labels for each topic.
custom_labels_	Custom labels for each topic as generated through .set_topic_labels.
topic_embeddings_	The embeddings for each topic if embedding_model was used.
representative_docs_	The representative documents for each topic if HDBSCAN is used
"""


print(topic_model.topics_[:10]) # acceder a los temas predichos
print(topic_model.probabilities_[:10]) # ver probabilidades
print(topic_model.c_tf_idf_[:10])
print(topic_model.topic_labels_[:10]) # ver las labels por defecto

### VISUALIZAR TOPICS
print(topic_model.visualize_topics())

print(topic_model.visualize_distribution(probs[200], min_probability=0.015)) # visualizar las probabilidades de los temas

#visualizar topic hierarchy
print(topic_model.visualize_hierarchy(top_n_topics=50))


#### VISUALIZAR TERMS
print(topic_model.visualize_barchart(top_n_topics=5))

# visualizar topic similarity
print(topic_model.visualize_heatmap(n_clusters=20, width=1000, height=1000))


# visualizar term score decline
print(topic_model.visualize_term_rank())


## topic representation

# update topics
print(topic_model.update_topics(docs, n_gram_range=(1, 2)))
print(topic_model.get_topic(0))   # We select topic that we viewed before




# TOPIC REDUCTION - reducimos el número de topics
print(topic_model.reduce_topics(docs, nr_topics=60))
print(topic_model.topics_)



## search topics - buscar topics similares , en este caso, que se parezcan a vehicle
similar_topics, similarity = topic_model.find_topics("vehicle", top_n=5); similar_topics
print(topic_model.get_topic(71))



### model serialization
topic_model.save("my_model")	# save model
my_model = BERTopic.load("my_model")	# load model7


### embedding models
# sentence transformers
topic_model = BERTopic(embedding_model="xlm-r-bert-base-nli-stsb-mean-tokens")
from sentence_transformers import SentenceTransformer

sentence_model = SentenceTransformer("distilbert-base-nli-mean-tokens", device="cpu")
topic_model = BERTopic(embedding_model=sentence_model, verbose=True)

